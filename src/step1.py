import os
import logging
from crewai import Agent, Task, Crew, Process
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI

# ç¦ç”¨æ‰€æœ‰é”™è¯¯æ—¥å¿—
logging.getLogger().setLevel(logging.CRITICAL)
os.environ["PYTHONWARNINGS"] = "ignore"


# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨OpenAIå…¼å®¹çš„API
os.environ["OPENAI_API_BASE"] = "http://localhost:11434/v1"
os.environ["OPENAI_API_KEY"] = "ollama"

# åˆå§‹åŒ–ä¸­æ–‡åµŒå…¥æ¨¡å‹
try:
    import torch
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
except ImportError:
    device = 'cpu'

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5",
    model_kwargs={'device': device},
    encode_kwargs={'normalize_embeddings': True}
)

# ä½¿ç”¨OpenAIå…¼å®¹çš„æ¥å£è¿æ¥Ollama
llm = ChatOpenAI(
    model="ollama/qwen3:4b",
    base_url="http://localhost:11434/v1",
    api_key="ollama",
    temperature=0.3
)



# å®šä¹‰æ™ºèƒ½ä½“
researcher = Agent(
    role='å¸‚åœºç ”ç©¶å‘˜',
    goal='æ”¶é›†ç”µåŠ¨æ±½è½¦å¸‚åœºæ•°æ®',
    backstory="ä¸“æ³¨äºæ–°èƒ½æºæ±½è½¦é¢†åŸŸçš„åˆ†æå¸ˆã€‚",
    memory=True,
    verbose=True,
    llm=llm,  # ä½¿ç”¨æ˜ç¡®åˆå§‹åŒ–çš„ Ollama LLM
    embeddings=embeddings
)

analyst = Agent(
    role='æ•°æ®åˆ†æå¸ˆ',
    goal='åˆ†æå¸‚åœºè¶‹åŠ¿',
    backstory="æ“…é•¿æ•°æ®å¤„ç†å’Œåˆ†æçš„ä¸“å®¶ã€‚",
    memory=True,
    verbose=True,
    llm=llm,
    embeddings=embeddings
)

# ç®€åŒ–ä»»åŠ¡ä»¥é¿å…é•¿æ—¶é—´è¿è¡Œ
research_task = Task(
    description="æ”¶é›†2024å¹´ç”µåŠ¨æ±½è½¦å¸‚åœºçš„åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸»è¦å‚å•†å’ŒæŠ€æœ¯å‘å±•ã€‚",
    agent=researcher,
    expected_output='ç®€è¦çš„å¸‚åœºæ¦‚è¿°'
)

analysis_task = Task(
    description="åˆ†æç”µåŠ¨æ±½è½¦å¸‚åœºçš„ä¸€ä¸ªä¸»è¦è¶‹åŠ¿ã€‚",
    agent=analyst,
    expected_output='ç®€è¦çš„è¶‹åŠ¿åˆ†æ'
)

# åˆ›å»ºå›¢é˜Ÿ
ev_crew = Crew(
    agents=[researcher, analyst],
    tasks=[research_task, analysis_task],
    process=Process.sequential,
    verbose=True,
    memory=True
)

# è¿è¡Œå›¢é˜Ÿ
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ç”µåŠ¨æ±½è½¦å¸‚åœºè°ƒç ”ä»»åŠ¡...")
    print("ğŸ“‹ ä½¿ç”¨æ¨¡å‹: qwen3:4b via Ollama")
    print("ğŸ”¤ åµŒå…¥æ¨¡å‹: BAAI/bge-small-zh-v1.5")
    
    # æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            print("âœ… Ollama æœåŠ¡è¿æ¥æ­£å¸¸")
            models = response.json().get('models', [])
            if models:
                print("ğŸ“¦ å·²å®‰è£…çš„æ¨¡å‹:")
                for model in models:
                    print(f"   - {model.get('name')}")
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å·²å®‰è£…çš„æ¨¡å‹")
                print("è¯·è¿è¡Œ: ollama pull qwen3:4b")
        else:
            print("âŒ Ollama æœåŠ¡å¼‚å¸¸")
    except requests.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡")
        print("è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ: ollama serve")
        exit(1)
    
    try:
        result = ev_crew.kickoff()
        print("\n" + "="*50)
        print("âœ… ä»»åŠ¡å®Œæˆï¼")
        print("="*50)
        print(result)
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        print("\nğŸ”§ æ•…éšœæ’é™¤æ­¥éª¤:")
        print("1. ç¡®ä¿ Ollama æœåŠ¡è¿è¡Œ: ollama serve")
        print("2. ç¡®è®¤æ¨¡å‹å·²ä¸‹è½½: ollama pull qwen3:4b")
        print("3. æ£€æŸ¥æ¨¡å‹åˆ—è¡¨: ollama list")
        print("4. æµ‹è¯•æ¨¡å‹: ollama run qwen3:4b 'ä½ å¥½'")